{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Language Technology - 3rd Tutorial on Word Embeddings (Word2Vec, FastText) using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    "----\n",
    "## Important Resources\n",
    "\n",
    "Language Technology Resources: https://eclass.aueb.gr/modules/document/index.php?course=INF210\n",
    "\n",
    "Python Official Documentation: https://docs.python.org/3.5/\n",
    "\n",
    "NLTK: https://www.nltk.org\n",
    "\n",
    "Gensim: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "Word2vec: https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Tutorial 5 Schedule\n",
    "\n",
    "**In order to familiarize ourselves with word embeddings, we have the following sections:**\n",
    "\n",
    "* **Train Word Embeddings**\n",
    "    * **Data Cleansing**\n",
    "    * **Lazy Load Data**\n",
    "    * **Train Word2Vec Model**\n",
    "    * **Train FastText Model**\n",
    "* **Use Word Embeddings**\n",
    "* **Train Word Embeddings with extra special tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "----\n",
    "# Background\n",
    "\n",
    "By the term word embedding in natural language processing (NLP), we describe a feature representation where words (tokens) or phrases (multi-token) from the vocabulary are mapped to vectors of real numbers. Distributed representations of words in a vector space help learning algorithms to achieve better performance in natural language processing tasks by grouping similar words. The ultimate leverage of such a technique is the transition from the traditional sparse features (i.e. one-hot vector representation) onto the dense vector space of common shared features.\n",
    "\n",
    "\n",
    "## Word2vec model sampling\n",
    "\n",
    "_\"You shall know a word by the company it keeps\" (Firth, J. R. 1957:11)_\n",
    "\n",
    "<img src=\"https://i.imgur.com/v34sAaT.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Word2vec model architecture\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-skip-gram.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "## Publicly available word embeddings\n",
    "\n",
    "There are various pre-trained word embeddings, trained on large corpora such as Google Vectors, which are trained over a vast corpus in Google News containing 100-billion words and finally producing a vocabulary of 3 million words.\n",
    "\n",
    "See here (https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models) for a list of every popular pre-trained word embeddings set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "----\n",
    "# Train Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In order to train word embeddings with gensim library, we have to prepare a list of sentences, which then will be fed to the word2vec model. We need to address this matter in a memory friendly way. In this tutotial we are going to follow a 2-step approach:\n",
    "\n",
    "\n",
    "## Data Cleansing - Preprocessing\n",
    "\n",
    "First we will clean (pre-process) our corpus by producing a new set of files in which every single line will be a sentence and sentence's words will be splitted by a simple space.\n",
    "\n",
    "**Python Code Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
      "18 BOOKS LOADED!\n",
      "austen-emma.txt DONE\n",
      "austen-persuasion.txt DONE\n",
      "austen-sense.txt DONE\n",
      "bible-kjv.txt DONE\n",
      "blake-poems.txt DONE\n",
      "bryant-stories.txt DONE\n",
      "burgess-busterbrown.txt DONE\n",
      "carroll-alice.txt DONE\n",
      "chesterton-ball.txt DONE\n",
      "chesterton-brown.txt DONE\n",
      "chesterton-thursday.txt DONE\n",
      "edgeworth-parents.txt DONE\n",
      "melville-moby_dick.txt DONE\n",
      "milton-paradise.txt DONE\n",
      "shakespeare-caesar.txt DONE\n",
      "shakespeare-hamlet.txt DONE\n",
      "shakespeare-macbeth.txt DONE\n",
      "whitman-leaves.txt DONE\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "import os\n",
    "import re\n",
    "\n",
    "PROCESSED_CORPUS_FOLDER = '/Users/kiddo/Desktop/word2vec_corpus'\n",
    "\n",
    "# Create new folder if it does not exist\n",
    "if not os.path.exists(PROCESSED_CORPUS_FOLDER):\n",
    "    os.mkdir(PROCESSED_CORPUS_FOLDER)\n",
    "\n",
    "# Get filenames for Gutenberg corpus\n",
    "file_ids = gutenberg.fileids()\n",
    "\n",
    "print(file_ids)\n",
    "print('{} BOOKS LOADED!'.format(len(file_ids)))\n",
    "\n",
    "# Iterate over filenames\n",
    "for filename in file_ids:\n",
    "    # Create new file including one sentence per line, tokenized on white-space \n",
    "    with open(os.path.join(PROCESSED_CORPUS_FOLDER, filename), 'w', encoding='utf-8') as output_file:\n",
    "        input_text = gutenberg.raw(filename)\n",
    "        for sentence in sent_tokenize(input_text):\n",
    "            sentence = re.sub('\\n+', ' ', sentence)\n",
    "            splitted_sentence = ' '.join([token.lower() for token in word_tokenize(sentence)]) + '\\n'\n",
    "            output_file.write(splitted_sentence)\n",
    "    print(filename + ' DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "----\n",
    "## Lazy Load Data\n",
    "\n",
    "Secondly we will create a generator, which will load the sentences in a lazy-loading fashion.\n",
    "\n",
    "**Python Code Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "class CorpusLoader(object):\n",
    "\n",
    "    def __init__(self,parent_folder=None):\n",
    "        self.dirname = parent_folder\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in glob.glob(os.path.join(self.dirname, '*.txt')):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Train Word2Vec Model\n",
    "\n",
    "Initialize word2vec model with specific parameters and train using the cleansed corpus and our generator.\n",
    "\n",
    "**Python Code Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START BUILDING VOCABULARY...\n",
      "VOCABULARY_SIZE:  11028\n",
      "START TRAINING...\n",
      "MODEL SAVED....\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# CREATE GENERATOR OBJECT TO STREAM SENTENCES\n",
    "sentences = CorpusLoader(parent_folder=PROCESSED_CORPUS_FOLDER)\n",
    "\n",
    "# CONFIGURE WORD2VEC MODEL\n",
    "model = Word2Vec(min_count=10, workers=cpu_count(), size=50, sg=1, window=5)\n",
    "\n",
    "# BUILD VOCABULARY FROM SENTENCES CONSIDERING MIN_COUNT\n",
    "print('START BUILDING VOCABULARY...')\n",
    "model.build_vocab(sentences)\n",
    "print('VOCABULARY_SIZE: ',len(model.wv.index2word))\n",
    "\n",
    "# TRAIN MODEL\n",
    "print('START TRAINING...')\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=10)\n",
    "\n",
    "model.wv.save_word2vec_format(os.path.join(PROCESSED_CORPUS_FOLDER, 'WORD2VEC.bin'), binary=True)\n",
    "print('MODEL SAVED....')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train FastText Model\n",
    "\n",
    "*Python Code Example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START BUILDING VOCABULARY...\n",
      "VOCABULARY_SIZE:  17212\n",
      "START TRAINING...\n",
      "MODEL SAVED....\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# CREATE GENERATOR OBJECT TO STREAM SENTENCES\n",
    "sentences = CorpusLoader(parent_folder=PROCESSED_CORPUS_FOLDER)\n",
    "\n",
    "# CONFIGURE WORD2VEC MODEL\n",
    "model = FastText(workers=cpu_count(), size=50, sg=1, window=5)\n",
    "\n",
    "# BUILD VOCABULARY FROM SENTENCES CONSIDERING MIN_COUNT\n",
    "print('START BUILDING VOCABULARY...')\n",
    "model.build_vocab(sentences)\n",
    "print('VOCABULARY_SIZE: ',len(model.wv.index2word))\n",
    "\n",
    "# TRAIN MODEL\n",
    "print('START TRAINING...')\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=10)\n",
    "\n",
    "model.save(os.path.join(PROCESSED_CORPUS_FOLDER, 'FASTTEXT.bin'))\n",
    "print('MODEL SAVED....')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "----\n",
    "# Use word embeddings\n",
    "\n",
    "**Python Code Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.10449308 -0.43362677  0.19262557 -0.2593011   0.17601196 -0.28308824\n",
      " -0.17368805 -0.1415757  -0.63802737  0.1822876   0.44478205  0.03437993\n",
      " -0.73681957  0.13764866 -0.18100835  0.14415143  0.45553008  0.11168631\n",
      " -0.32792276  0.24922058  0.55671835  0.03754492  0.22137128 -0.23959972\n",
      "  0.13231476  1.1833037   0.49699035  0.09289576 -0.12476934 -0.14078905\n",
      " -0.05570467 -0.409427    0.05100378  0.44504216  0.3083692   0.20310277\n",
      "  0.3315212  -0.04255746  0.04521555 -0.01845262 -0.26361692 -0.0837402\n",
      " -0.38756976  0.44985986 -0.36964557 -0.14428945  0.6458716  -0.13912514\n",
      " -0.10920626 -0.08730115]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# LOAD WORD2VEC LOOKUP TABLE\n",
    "w2v_model = KeyedVectors.load_word2vec_format(os.path.join(PROCESSED_CORPUS_FOLDER, 'WORD2VEC.bin'), binary=True)\n",
    "\n",
    "print(w2v_model['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "night   \n",
      "--------------------\n",
      "day            | 0.80  \n",
      "afternoon      | 0.76  \n",
      "morning        | 0.73  \n",
      "autumn         | 0.72  \n",
      "waking         | 0.71  \n",
      "awake          | 0.70  \n",
      "musing         | 0.70  \n",
      "dawn           | 0.70  \n",
      "winter         | 0.69  \n",
      "noon           | 0.68  \n",
      "\n",
      "dog     \n",
      "--------------------\n",
      "thief          | 0.76  \n",
      "pet            | 0.75  \n",
      "todhunter      | 0.72  \n",
      "lion           | 0.72  \n",
      "alligator      | 0.71  \n",
      "boy            | 0.71  \n",
      "fox            | 0.70  \n",
      "pig            | 0.69  \n",
      "bullet         | 0.69  \n",
      "farmer         | 0.68  \n",
      "\n",
      "umbrella\n",
      "--------------------\n",
      "frock-coat     | 0.91  \n",
      "jacket         | 0.89  \n",
      "waistcoat      | 0.87  \n",
      "collar         | 0.87  \n",
      "handkerchief   | 0.87  \n",
      "man's          | 0.87  \n",
      "moustache      | 0.86  \n",
      "clerical       | 0.86  \n",
      "slouched       | 0.86  \n",
      "tight          | 0.86  \n",
      "\n",
      "shoe    \n",
      "--------------------\n",
      "skirt          | 0.77  \n",
      "mane           | 0.76  \n",
      "foot           | 0.76  \n",
      "neck           | 0.76  \n",
      "vesture        | 0.75  \n",
      "heels          | 0.75  \n",
      "bedchamber     | 0.75  \n",
      "hip            | 0.75  \n",
      "walking-stick  | 0.74  \n",
      "lap            | 0.74  \n",
      "\n",
      "man     \n",
      "--------------------\n",
      "gentleman      | 0.79  \n",
      "person         | 0.76  \n",
      "woman          | 0.73  \n",
      "citizen        | 0.68  \n",
      "quaker         | 0.67  \n",
      "idiot          | 0.67  \n",
      "fellow         | 0.67  \n",
      "creature       | 0.67  \n",
      "fool           | 0.66  \n",
      "fairy          | 0.66  \n",
      "\n",
      "woman   \n",
      "--------------------\n",
      "child          | 0.77  \n",
      "person         | 0.74  \n",
      "man            | 0.73  \n",
      "girl           | 0.72  \n",
      "young          | 0.70  \n",
      "maid           | 0.69  \n",
      "surety         | 0.69  \n",
      "selina         | 0.69  \n",
      "widow          | 0.68  \n",
      "wife           | 0.67  \n",
      "\n",
      "boat    \n",
      "--------------------\n",
      "oar            | 0.80  \n",
      "squall         | 0.80  \n",
      "harpoon        | 0.79  \n",
      "sideways       | 0.79  \n",
      "crew           | 0.78  \n",
      "windward       | 0.78  \n",
      "deck           | 0.78  \n",
      "flank          | 0.78  \n",
      "coils          | 0.77  \n",
      "hoist          | 0.77  \n",
      "\n",
      "love    \n",
      "--------------------\n",
      "delight        | 0.71  \n",
      "divine         | 0.69  \n",
      "reverence      | 0.69  \n",
      "loves          | 0.69  \n",
      "goodness       | 0.69  \n",
      "truths         | 0.68  \n",
      "dearest        | 0.68  \n",
      "indulgent      | 0.68  \n",
      "admire         | 0.67  \n",
      "brotherly      | 0.67  \n",
      "\n",
      "george  \n",
      "--------------------\n",
      "davis          | 0.78  \n",
      "bunger         | 0.78  \n",
      "carstairs      | 0.77  \n",
      "phoo           | 0.77  \n",
      "t.             | 0.77  \n",
      "goose          | 0.76  \n",
      "-              | 0.75  \n",
      "renard         | 0.75  \n",
      "signor         | 0.74  \n",
      "surgeon        | 0.74  \n",
      "\n",
      "5       \n",
      "--------------------\n",
      "10             | 0.84  \n",
      "august         | 0.84  \n",
      "6              | 0.84  \n",
      "9              | 0.83  \n",
      "7              | 0.83  \n",
      "8              | 0.81  \n",
      "x              | 0.81  \n",
      "v              | 0.80  \n",
      "january        | 0.79  \n",
      "13             | 0.79  \n",
      "\n",
      "587      NOT FOUND\n"
     ]
    }
   ],
   "source": [
    "# SEARCH (LOOKUP) WORD EMBEDDINGS\n",
    "words = ['night', 'dog', 'umbrella', 'shoe', 'man', 'woman', 'boat', 'love', 'george', '5', '587']\n",
    "\n",
    "# FIND MOST SIMILAR WORDS\n",
    "for word in words:\n",
    "    if word in w2v_model:\n",
    "        record = '{:8}\\n'.format(word)\n",
    "        record += '--------------------\\n'\n",
    "        for sim_word, sim_score in w2v_model.most_similar(positive=[word]):\n",
    "            record += '{:15}| {:.2f}  \\n'.format(sim_word, sim_score)\n",
    "        print(record)\n",
    "    else:\n",
    "        print('{:8} NOT FOUND'.format(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7261805\n",
      "0.52326065\n",
      "0.43960875\n"
     ]
    }
   ],
   "source": [
    "# SIMILARITY MEASURE\n",
    "print(w2v_model.similarity('man', 'woman'))\n",
    "print(w2v_model.similarity('man', 'pig'))\n",
    "print(w2v_model.similarity('man', 'chair'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " FastText in contrast with Word2Vec does not rely on a fixed vocabulary, so we need to load the actual trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "night   \n",
      "--------------------\n",
      "nightfall      | 0.84  \n",
      "daylight       | 0.80  \n",
      "nightmare      | 0.79  \n",
      "noonday        | 0.79  \n",
      "midnight       | 0.79  \n",
      "yesternight    | 0.78  \n",
      "morning        | 0.77  \n",
      "day            | 0.76  \n",
      "noon           | 0.75  \n",
      "tonight        | 0.75  \n",
      "\n",
      "dog     \n",
      "--------------------\n",
      "cage           | 0.80  \n",
      "yarman         | 0.80  \n",
      "todhunter      | 0.78  \n",
      "cradle         | 0.77  \n",
      "tomb           | 0.77  \n",
      "chimney        | 0.76  \n",
      "pig            | 0.75  \n",
      "chimney-sweeper| 0.75  \n",
      "oyster         | 0.75  \n",
      "banker         | 0.75  \n",
      "\n",
      "umbrella\n",
      "--------------------\n",
      "frock-coat     | 0.85  \n",
      "dressing-gown  | 0.85  \n",
      "shawl          | 0.85  \n",
      "waistcoat      | 0.83  \n",
      "negro          | 0.83  \n",
      "jacket         | 0.83  \n",
      "turkish        | 0.82  \n",
      "sombre         | 0.82  \n",
      "turban         | 0.82  \n",
      "arm-chair      | 0.81  \n",
      "\n",
      "shoe    \n",
      "--------------------\n",
      "foot           | 0.81  \n",
      "foote          | 0.80  \n",
      "shoes          | 0.79  \n",
      "toe            | 0.79  \n",
      "shod           | 0.78  \n",
      "cloak          | 0.78  \n",
      "loafe          | 0.78  \n",
      "beaver         | 0.78  \n",
      "bragge         | 0.77  \n",
      "loose          | 0.77  \n",
      "\n",
      "man     \n",
      "--------------------\n",
      "person         | 0.81  \n",
      "gentleman      | 0.79  \n",
      "manxman        | 0.79  \n",
      "oman           | 0.78  \n",
      "woman          | 0.78  \n",
      "gentlewoman    | 0.78  \n",
      "madman         | 0.75  \n",
      "gentlemanlike  | 0.72  \n",
      "suliman        | 0.72  \n",
      "leper          | 0.71  \n",
      "\n",
      "woman   \n",
      "--------------------\n",
      "child          | 0.82  \n",
      "bondwoman      | 0.82  \n",
      "gentlewoman    | 0.79  \n",
      "man            | 0.78  \n",
      "person         | 0.78  \n",
      "girl           | 0.74  \n",
      "milk-woman     | 0.74  \n",
      "oman           | 0.74  \n",
      "lover          | 0.71  \n",
      "husbandman     | 0.71  \n",
      "\n",
      "boat    \n",
      "--------------------\n",
      "whale-boat     | 0.84  \n",
      "quarter-deck   | 0.83  \n",
      "deck           | 0.81  \n",
      "crew           | 0.81  \n",
      "ship           | 0.81  \n",
      "steersman      | 0.79  \n",
      "steer          | 0.79  \n",
      "stern          | 0.78  \n",
      "boats          | 0.78  \n",
      "ahead          | 0.78  \n",
      "\n",
      "love    \n",
      "--------------------\n",
      "lover          | 0.79  \n",
      "delight        | 0.76  \n",
      "lovingkindness | 0.74  \n",
      "loves          | 0.74  \n",
      "loveliness     | 0.72  \n",
      "behalf         | 0.72  \n",
      "deserve        | 0.71  \n",
      "delighteth     | 0.71  \n",
      "plead          | 0.71  \n",
      "delights       | 0.71  \n",
      "\n",
      "george  \n",
      "--------------------\n",
      "georgia        | 0.86  \n",
      "massa          | 0.80  \n",
      "surgeon        | 0.78  \n",
      "p.             | 0.78  \n",
      "t.             | 0.78  \n",
      "bulkington     | 0.78  \n",
      "eustache       | 0.77  \n",
      "st.            | 0.77  \n",
      "hudson         | 0.77  \n",
      "capstan        | 0.76  \n",
      "\n",
      "5       \n",
      "--------------------\n",
      "6              | 0.87  \n",
      "9              | 0.84  \n",
      "xiv            | 0.82  \n",
      "7              | 0.82  \n",
      "8              | 0.82  \n",
      "august         | 0.82  \n",
      "x              | 0.81  \n",
      "v              | 0.81  \n",
      "50             | 0.81  \n",
      "xv             | 0.80  \n",
      "\n",
      "dog-walker\n",
      "--------------------\n",
      "walker         | 0.87  \n",
      "talker         | 0.81  \n",
      "befriend       | 0.79  \n",
      "adhere         | 0.78  \n",
      "'one           | 0.77  \n",
      "adjure         | 0.77  \n",
      "conjurer       | 0.77  \n",
      "madman         | 0.77  \n",
      "liuing         | 0.76  \n",
      "truer          | 0.75  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasttext_model = FastText.load(os.path.join(PROCESSED_CORPUS_FOLDER, 'FASTTEXT.bin'))\n",
    "\n",
    "# SEARCH FASTTEXT MODEL\n",
    "words = ['night', 'dog', 'umbrella', 'shoe', 'man', 'woman', 'boat', 'love', 'george', '5', 'dog-walker']\n",
    "\n",
    "# FIND MOST SIMILAR WORDS\n",
    "for word in words:\n",
    "    record = '{:8}\\n'.format(word)\n",
    "    record += '--------------------\\n'\n",
    "    for sim_word, sim_score in fasttext_model.wv.most_similar(positive=[word]):\n",
    "        record += '{:15}| {:.2f}  \\n'.format(sim_word, sim_score)\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2Vec Model with extra special tokens\n",
    "\n",
    "(a) Normalize numbers, (b) Create unknown token, (c) Preserve newline character\n",
    "\n",
    "**Python Code Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
      "18 BOOKS LOADED!\n",
      "10 MOST COMMON TOKENS: [(',', 192339), ('☃️', 153371), ('the', 133513), ('and', 95296), ('.', 76625), ('of', 71206), ('to', 47692), ('a', 33804), ('in', 33480), ('i', 29973)]\n",
      "austen-emma.txt DONE\n",
      "austen-persuasion.txt DONE\n",
      "austen-sense.txt DONE\n",
      "bible-kjv.txt DONE\n",
      "blake-poems.txt DONE\n",
      "bryant-stories.txt DONE\n",
      "burgess-busterbrown.txt DONE\n",
      "carroll-alice.txt DONE\n",
      "chesterton-ball.txt DONE\n",
      "chesterton-brown.txt DONE\n",
      "chesterton-thursday.txt DONE\n",
      "edgeworth-parents.txt DONE\n",
      "melville-moby_dick.txt DONE\n",
      "milton-paradise.txt DONE\n",
      "shakespeare-caesar.txt DONE\n",
      "shakespeare-hamlet.txt DONE\n",
      "shakespeare-macbeth.txt DONE\n",
      "whitman-leaves.txt DONE\n",
      "START BUILDING VOCABULARY...\n",
      "VOCABULARY_SIZE:  10047\n",
      "START TRAINING...\n",
      "MODEL SAVED....\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create new folder if it does not exist\n",
    "if not os.path.exists(PROCESSED_CORPUS_FOLDER):\n",
    "    os.mkdir(PROCESSED_CORPUS_FOLDER)\n",
    "\n",
    "# Get filenames for Gutenberg corpus\n",
    "file_ids = gutenberg.fileids()\n",
    "\n",
    "print(file_ids)\n",
    "print('{} BOOKS LOADED!'.format(len(file_ids)))\n",
    "\n",
    "tokens = []\n",
    "\n",
    "# Iterate over filenames\n",
    "for filename in file_ids:\n",
    "    # Create new file including one sentence per line, tokenized on white-space \n",
    "    input_text = gutenberg.raw(filename)\n",
    "    for sentence in sent_tokenize(input_text):\n",
    "        # Preserve newline with snowmam :D\n",
    "        sentence = re.sub('\\n+', ' ☃️ ', sentence)\n",
    "        tokens.extend([token.lower() if not re.search('[0-9]', token) else re.sub('[0-9]', 'D', token.lower())\n",
    "                       for token in word_tokenize(sentence)])\n",
    "print('TOTAL TOKENS IN CORPUS: {}'.format(len(tokens)))\n",
    "vocab = Counter(tokens)\n",
    "\n",
    "print('10 MOST COMMON TOKENS: {}'.format(vocab.most_common(10)))\n",
    "\n",
    "# ELIMINATE RARE WORDS (<10)\n",
    "vocab = {k:v for k,v in vocab.most_common() if v >= 10}\n",
    "\n",
    "# Iterate over filenames\n",
    "for filename in file_ids:\n",
    "    # Create new file including one sentence per line, tokenized on white-space \n",
    "    with open(os.path.join(PROCESSED_CORPUS_FOLDER, filename), 'w', encoding='utf-8') as output_file:\n",
    "        input_text = gutenberg.raw(filename)\n",
    "        for sentence in sent_tokenize(input_text):\n",
    "            sentence = re.sub('\\n+', ' ☃️ ', sentence)\n",
    "            normalized_tokens = []\n",
    "            for token in word_tokenize(sentence):\n",
    "                if token == '☃️':\n",
    "                    normalized_tokens.append('#NEWLINE#')\n",
    "                elif re.search('[0-9]', token):\n",
    "                    token_norm = re.sub('[0-9]', 'D', token.lower())\n",
    "                    if token_norm in vocab:\n",
    "                        normalized_tokens.append(token_norm)\n",
    "                    else:\n",
    "                        normalized_tokens.append('#UNK#')\n",
    "                else:\n",
    "                    if token.lower() in vocab:\n",
    "                        normalized_tokens.append(token.lower())\n",
    "                    else:\n",
    "                        normalized_tokens.append('#UNK#')\n",
    "            splitted_sentence = ' '.join(normalized_tokens) + '\\n'\n",
    "            output_file.write(splitted_sentence)\n",
    "    print(filename + ' DONE')\n",
    "    \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# CREATE GENERATOR OBJECT TO STREAM SENTENCES\n",
    "sentences = CorpusLoader(parent_folder=PROCESSED_CORPUS_FOLDER)\n",
    "\n",
    "# CONFIGURE WORD2VEC MODEL\n",
    "model = Word2Vec(workers=cpu_count(), size=50, sg=1, window=5)\n",
    "\n",
    "# BUILD VOCABULARY FROM SENTENCES CONSIDERING MIN_COUNT\n",
    "print('START BUILDING VOCABULARY...')\n",
    "model.build_vocab(sentences)\n",
    "print('VOCABULARY_SIZE: ',len(model.wv.index2word))\n",
    "\n",
    "# TRAIN MODEL\n",
    "print('START TRAINING...')\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=10)\n",
    "\n",
    "model.wv.save_word2vec_format(os.path.join(PROCESSED_CORPUS_FOLDER, 'WORD2VEC_UNK.bin'), binary=True)\n",
    "print('MODEL SAVED....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#UNK#   \n",
      "--------------------\n",
      "#NEWLINE#      | 0.89  \n",
      ",              | 0.86  \n",
      "precision      | 0.81  \n",
      "ramadan        | 0.79  \n",
      "motto          | 0.78  \n",
      "preliminary    | 0.78  \n",
      "and            | 0.77  \n",
      "practically    | 0.77  \n",
      "correctly      | 0.77  \n",
      "obviously      | 0.77  \n",
      "\n",
      "#NEWLINE#\n",
      "--------------------\n",
      ",              | 0.93  \n",
      "#UNK#          | 0.89  \n",
      "and            | 0.88  \n",
      ";              | 0.86  \n",
      "that           | 0.83  \n",
      "to             | 0.81  \n",
      "--             | 0.80  \n",
      "the            | 0.80  \n",
      "of             | 0.78  \n",
      "curate         | 0.77  \n",
      "\n",
      "DD      \n",
      "--------------------\n",
      "chapter        | 0.86  \n",
      "DDD            | 0.80  \n",
      "december       | 0.75  \n",
      "DDDD           | 0.74  \n",
      "iii            | 0.74  \n",
      "a.d.           | 0.74  \n",
      "january        | 0.73  \n",
      "v              | 0.72  \n",
      "august         | 0.71  \n",
      "vi             | 0.71  \n",
      "\n",
      "DDDD    \n",
      "--------------------\n",
      "december       | 0.85  \n",
      "a.d.           | 0.84  \n",
      "june           | 0.81  \n",
      "august         | 0.80  \n",
      "]              | 0.79  \n",
      "iii            | 0.79  \n",
      "[              | 0.78  \n",
      "chapter        | 0.77  \n",
      "octavo         | 0.77  \n",
      "actus          | 0.77  \n",
      "\n",
      "D,DDD    NOT FOUND\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# LOAD WORD2VEC LOOKUP TABLE\n",
    "w2v_model = KeyedVectors.load_word2vec_format(os.path.join(PROCESSED_CORPUS_FOLDER, 'WORD2VEC_UNK.bin'), binary=True)\n",
    "\n",
    "special_tokens = ['#UNK#', '#NEWLINE#', 'DD', 'DDDD', 'D,DDD']\n",
    "\n",
    "# FIND MOST SIMILAR WORDS\n",
    "for word in special_tokens:\n",
    "    if word in w2v_model:\n",
    "        record = '{:8}\\n'.format(word)\n",
    "        record += '--------------------\\n'\n",
    "        for sim_word, sim_score in w2v_model.most_similar(positive=[word]):\n",
    "            record += '{:15}| {:.2f}  \\n'.format(sim_word, sim_score)\n",
    "        print(record)\n",
    "    else:\n",
    "        print('{:8} NOT FOUND'.format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
